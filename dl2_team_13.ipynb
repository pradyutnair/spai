{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Sem-SPAI: Semantic-enhanced SPAI Training and Testing\n",
    "\n",
    "This notebook contains the training and testing commands for the semantic-enhanced SPAI model based on our job configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÅ Setup: Model Directories and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and data paths configuration\n",
    "import os\n",
    "\n",
    "# Base path\n",
    "BASE_PATH = \"/home/scur2605/spai\"\n",
    "CONFIG_PATH = os.path.join(BASE_PATH, \"configs/spai.yaml\")\n",
    "\n",
    "# Training data paths\n",
    "TRAIN_DATA = {\n",
    "    \"ldm_lsun_subset\": os.path.join(BASE_PATH, \"data/ldm_lsun_train_val_subset.csv\"),\n",
    "    \"chameleon\": os.path.join(BASE_PATH, \"data/chameleon_dataset_split.csv\"),\n",
    "    \"ldm_subset_10pct\": os.path.join(BASE_PATH, \"data/train/ldm_train_val_subset_10pct.csv\")\n",
    "}\n",
    "\n",
    "# Model paths\n",
    "MODEL_PATHS = {\n",
    "    \"trained_model\": os.path.join(BASE_PATH, \"output/LSUN_RESIDUAL_ORIGINAL/finetune/first_run/ckpt_epoch_6.pth\"),\n",
    "    \"spai_pretrained\": os.path.join(BASE_PATH, \"weights/spai.pth\"),\n",
    "    \"output_dir\": os.path.join(BASE_PATH, \"output/LSUN_RESIDUAL_ORIGINAL\")\n",
    "}\n",
    "\n",
    "# Test datasets\n",
    "TEST_DATASETS = {\n",
    "    \"dalle2\": os.path.join(BASE_PATH, \"data/test_set_dalle2.csv\"),\n",
    "    \"dalle3\": os.path.join(BASE_PATH, \"data/test_set_dalle3.csv\"),\n",
    "    \"gigagan\": os.path.join(BASE_PATH, \"data/test_set_gigagan.csv\"),\n",
    "    \"sd1_4\": os.path.join(BASE_PATH, \"data/test_set_sd1_4.csv\"),\n",
    "    \"sd3\": os.path.join(BASE_PATH, \"data/test_set_sd3.csv\"),\n",
    "    \"sdxl\": os.path.join(BASE_PATH, \"data/test_set_sdxl.csv\"),\n",
    "    \"flux\": os.path.join(BASE_PATH, \"data/test_set_flux.csv\"),\n",
    "    \"midjourney\": os.path.join(BASE_PATH, \"data/test_set_midjourney-v6.1.csv\")\n",
    "}\n",
    "\n",
    "print(\"üèóÔ∏è Model Paths:\")\n",
    "for key, path in MODEL_PATHS.items():\n",
    "    print(f\"  {key}: {path}\")\n",
    "\n",
    "print(\"\\nüìä Training Datasets:\")\n",
    "for key, path in TRAIN_DATA.items():\n",
    "    print(f\"  {key}: {path}\")\n",
    "\n",
    "print(\"\\nüß™ Test Datasets:\")\n",
    "for key, path in TEST_DATASETS.items():\n",
    "    print(f\"  {key}: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Training Commands\n",
    "\n",
    "Based on `jobs/semantic/train.job` - Training the semantic-enhanced SPAI model with late fusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment setup commands (run these first)\n",
    "setup_commands = f\"\"\"\n",
    "# Set environment variables\n",
    "export PYTHONPATH=<your_path_here>\n",
    "export NEPTUNE_API_TOKEN=\"<your_token_here>\"\n",
    "export NEPTUNE_PROJECT=\"<your_project_here>\"\n",
    "\n",
    "# Activate conda environment\n",
    "conda activate spai_2\n",
    "\n",
    "# Change to project directory\n",
    "cd {BASE_PATH}\n",
    "\"\"\"\n",
    "\n",
    "print(\"üîß Environment Setup:\")\n",
    "print(setup_commands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training command - Semantic-Enhanced SPAI with Late Fusion\n",
    "print(\"üéì Main Training Command:\")\n",
    "print(\"\"\"\n",
    "python -m spai train \\\\\n",
    "--cfg \"./configs/spai.yaml\" \\\\\n",
    "--batch-size 256 \\\\\n",
    "--data-path \"/home/scur2605/spai/data/ldm_lsun_train_val_subset.csv\" \\\\\n",
    "--csv-root-dir \"/home/scur2605/spai/data/train\" \\\\\n",
    "--output \"./output/LSUN_RESIDUAL_ORIGINAL\" \\\\\n",
    "--tag \"first_run\" \\\\\n",
    "--data-workers 4 \\\\\n",
    "--save-all \\\\\n",
    "--amp-opt-level \"O0\" \\\\\n",
    "--opt \"TRAIN.EPOCHS\" \"10\" \\\\\n",
    "--opt \"DATA.TEST_PREFETCH_FACTOR\" \"1\" \\\\\n",
    "--opt \"DATA.VAL_BATCH_SIZE\" \"256\" \\\\\n",
    "--opt \"MODEL.FEATURE_EXTRACTION_BATCH\" \"400\" \\\\\n",
    "--opt \"PRINT_FREQ\" \"2\" \\\\\n",
    "--opt \"MODEL.SEMANTIC_CONTEXT.SPAI_INPUT_SIZE\" \"[224, 224]\"\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Testing Commands\n",
    "\n",
    "Based on `jobs/semantic/test.job` - Comprehensive evaluation across multiple datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing commands for key datasets\n",
    "MODEL_PATH = \"/home/scur2605/spai/output/LSUN_RESIDUAL_ORIGINAL/finetune/first_run/ckpt_epoch_6.pth\"\n",
    "\n",
    "# Base test command template\n",
    "base_test_cmd = \"\"\"\n",
    "python -m spai test \\\\\n",
    "--cfg \"./configs/spai.yaml\" \\\\\n",
    "--batch-size 10 \\\\\n",
    "--model \"{model_path}\" \\\\\n",
    "--output \"./output/semantic_test\" \\\\\n",
    "--tag \"spai\" \\\\\n",
    "--opt \"MODEL.PATCH_VIT.MINIMUM_PATCHES\" \"4\" \\\\\n",
    "--opt \"DATA.NUM_WORKERS\" \"8\" \\\\\n",
    "--opt \"MODEL.FEATURE_EXTRACTION_BATCH\" \"400\" \\\\\n",
    "--opt \"DATA.TEST_PREFETCH_FACTOR\" \"1\" \\\\\n",
    "--test-csv \"{test_csv}\" \\\\\n",
    "--opt \"PRINT_FREQ\" \"2\" \\\\\n",
    "--opt \"MODEL.SEMANTIC_CONTEXT.HIDDEN_DIMS\" \"[512]\" \\\\\n",
    "--opt \"MODEL.SEMANTIC_CONTEXT.SPAI_INPUT_SIZE\" \"[1024, 1024]\"\n",
    "\"\"\"\n",
    "\n",
    "# Example: DALLE-2 Testing\n",
    "print(\"üìä Testing on DALLE-2:\")\n",
    "print(base_test_cmd.format(\n",
    "    model_path=MODEL_PATH,\n",
    "    test_csv=\"/home/scur2605/spai/data/test_set_dalle2.csv\"\n",
    "))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All test datasets commands\n",
    "test_datasets = [\n",
    "    (\"DALLE-3\", \"test_set_dalle3.csv\"),\n",
    "    (\"GigaGAN\", \"test_set_gigagan.csv\"), \n",
    "    (\"SD1.4\", \"test_set_sd1_4.csv\"),\n",
    "    (\"SD3\", \"test_set_sd3.csv\"),\n",
    "    (\"SDXL\", \"test_set_sdxl.csv\"),\n",
    "    (\"Flux\", \"test_set_flux.csv\"),\n",
    "    (\"Midjourney\", \"test_set_midjourney-v6.1.csv\")\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing Commands for All Datasets:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for dataset_name, csv_file in test_datasets:\n",
    "    print(f\"\\nüìä Testing on {dataset_name}:\")\n",
    "    test_csv_path = f\"/home/scur2605/spai/data/{csv_file}\"\n",
    "    print(base_test_cmd.format(\n",
    "        model_path=MODEL_PATH,\n",
    "        test_csv=test_csv_path\n",
    "    ))\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Job Submission Commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SLURM job submission commands\n",
    "print(\"üìä SLURM Job Submission Commands:\")\n",
    "print(\"\"\"\n",
    "# Submit training job (GPU H100, 12 hours, 180GB RAM)\n",
    "sbatch jobs/semantic/train.job\n",
    "\n",
    "# Submit testing job (GPU H100, 5 hours, 180GB RAM)  \n",
    "sbatch jobs/semantic/test.job\n",
    "\n",
    "# Check job status\n",
    "squeue -u $USER\n",
    "\n",
    "# Check job output\n",
    "tail -f jobs/outputs/semantic/train_simple_*.out\n",
    "tail -f jobs/outputs/semantic/test-pope*.out\n",
    "\n",
    "# Cancel job if needed\n",
    "scancel <JOB_ID>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configuration Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration summary\n",
    "print(\"‚öôÔ∏è Model Configuration Summary:\")\n",
    "print(\"=\" * 40)\n",
    "print(\"\"\"\n",
    "Training Parameters:\n",
    "  ‚Ä¢ Batch Size: 256\n",
    "  ‚Ä¢ Epochs: 10\n",
    "  ‚Ä¢ Data Workers: 4\n",
    "  ‚Ä¢ AMP Level: O0 (no mixed precision)\n",
    "  ‚Ä¢ SPAI Input Size: [224, 224]\n",
    "\n",
    "Testing Parameters:\n",
    "  ‚Ä¢ Batch Size: 10\n",
    "  ‚Ä¢ Data Workers: 8\n",
    "  ‚Ä¢ SPAI Input Size: [1024, 1024]\n",
    "  ‚Ä¢ Hidden Dims: [512]\n",
    "  ‚Ä¢ Min Patches: 4\n",
    "\n",
    "Hardware Requirements:\n",
    "  ‚Ä¢ GPU: H100\n",
    "  ‚Ä¢ Memory: 180GB\n",
    "  ‚Ä¢ CPUs: 16\n",
    "  ‚Ä¢ Training Time: 12 hours\n",
    "  ‚Ä¢ Testing Time: 5 hours\n",
    "\n",
    "Key Features:\n",
    "  ‚Ä¢ Late fusion architecture with residual connections\n",
    "  ‚Ä¢ Frozen SPAI and ConvNeXt-XXL backbones\n",
    "  ‚Ä¢ Semantic features projected to 256 dimensions\n",
    "  ‚Ä¢ Multi-dataset evaluation capability\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
