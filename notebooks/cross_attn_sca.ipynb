{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8042238",
   "metadata": {},
   "source": [
    "## Adding semantic context to SPAI: cross attention before/after the SCA module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68ff0634",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\agata\\miniconda3\\envs\\spai\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "from functools import partial\n",
    "\n",
    "# imports and constants\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import torch\n",
    "\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "from spai.models import sid\n",
    "from spai.models import vision_transformer\n",
    "from spai.models import backbones\n",
    "\n",
    "\n",
    "IMAGE_PATH = \"./../data/images/fake_example.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01a7d0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### OLD CODE ####\n",
    "# def show_images_in_row(images, titles=None, figsize=(15, 5)):\n",
    "#     \"\"\"\n",
    "#     Displays a list of image arrays in a single row.\n",
    "\n",
    "#     Args:\n",
    "#         images (list): List of image data (PIL.Image or NumPy arrays).\n",
    "#         titles (list, optional): Optional list of titles for each image.\n",
    "#         figsize (tuple): Size of the entire figure (width, height).\n",
    "#     \"\"\"\n",
    "#     num_images = len(images)\n",
    "#     fig, axes = plt.subplots(1, num_images, figsize=figsize)\n",
    "\n",
    "#     if num_images == 1:\n",
    "#         axes = [axes]\n",
    "\n",
    "#     for i, (ax, img) in enumerate(zip(axes, images)):\n",
    "#         ax.imshow(img)\n",
    "#         ax.axis(\"off\")\n",
    "#         if titles and i < len(titles):\n",
    "#             ax.set_title(titles[i], fontsize=10)\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "# img_og = Image.open(IMAGE_PATH)\n",
    "# # resize to 224x224\n",
    "# img_resized = img_og.resize((224, 224))\n",
    "\n",
    "# show_images_in_row(\n",
    "#     [img_og, img_resized], titles=[\"Original Image\", \"224x224\"], figsize=(8, 4)\n",
    "# )\n",
    "\n",
    "#############################################################\n",
    "# from torchvision import transforms\n",
    "# from spai.models.backbones import CLIPBackbone\n",
    "\n",
    "# clip_encoder = CLIPBackbone()\n",
    "\n",
    "# # NOTE: we might need to normalize according to clip mean/std\n",
    "# img_tensor = transforms.ToTensor()(img_resized).unsqueeze(0)\n",
    "# print(f\"Image tensor shape: {img_tensor.shape}\") # ([1, 3, 224, 224]\n",
    "\n",
    "# img_encoding = clip_encoder(img_tensor)\n",
    "# # print encoding shape\n",
    "# print(f\"Encoding shape: {img_encoding.shape}\") # ([1, 12, 196, 768])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb78b45",
   "metadata": {},
   "source": [
    "### Test: loading the model with the semantic changes and the forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b1ac62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward with different backbones\n",
    "backbone_vits = [\n",
    "    # vision_transformer.VisionTransformer(\n",
    "    #     img_size=224,\n",
    "    #     patch_size=16,\n",
    "    #     in_chans=3,\n",
    "    #     num_classes=2,\n",
    "    #     embed_dim=768,\n",
    "    #     depth=12,\n",
    "    #     num_heads=12,\n",
    "    #     mlp_ratio=4,\n",
    "    #     qkv_bias=True,\n",
    "    #     drop_rate=0.1,\n",
    "    #     drop_path_rate=0.1,\n",
    "    #     norm_layer=partial(torch.nn.LayerNorm, eps=1e-6),\n",
    "    #     init_values=0.1,\n",
    "    #     use_abs_pos_emb=True,\n",
    "    #     use_rel_pos_bias=False,\n",
    "    #     use_shared_rel_pos_bias=False,\n",
    "    #     use_mean_pooling=False,\n",
    "    #     use_intermediate_layers=True,\n",
    "    #     intermediate_layers=tuple(range(12)),\n",
    "    #     return_features=True,\n",
    "    # ),\n",
    "    backbones.CLIPBackbone().cpu(),\n",
    "    # backbones.DINOv2Backbone().cpu(),\n",
    "]\n",
    "\n",
    "vit = backbone_vits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87c76ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\agata\\AppData\\Local\\Temp\\ipykernel_21648\\3944651413.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_weights = torch.load(model_weights_path, map_location=\"cpu\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PatchBasedMFViT(\n",
       "  (mfvit): MFViT(\n",
       "    (vit): CLIPBackbone(\n",
       "      (clip): CLIP(\n",
       "        (visual): VisionTransformer(\n",
       "          (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
       "          (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (transformer): Transformer(\n",
       "            (resblocks): Sequential(\n",
       "              (0): ResidualAttentionBlock(\n",
       "                (attn): MultiheadAttention(\n",
       "                  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "                )\n",
       "                (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Sequential(\n",
       "                  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                  (gelu): QuickGELU()\n",
       "                  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                )\n",
       "                (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (1): ResidualAttentionBlock(\n",
       "                (attn): MultiheadAttention(\n",
       "                  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "                )\n",
       "                (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Sequential(\n",
       "                  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                  (gelu): QuickGELU()\n",
       "                  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                )\n",
       "                (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (2): ResidualAttentionBlock(\n",
       "                (attn): MultiheadAttention(\n",
       "                  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "                )\n",
       "                (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Sequential(\n",
       "                  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                  (gelu): QuickGELU()\n",
       "                  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                )\n",
       "                (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (3): ResidualAttentionBlock(\n",
       "                (attn): MultiheadAttention(\n",
       "                  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "                )\n",
       "                (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Sequential(\n",
       "                  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                  (gelu): QuickGELU()\n",
       "                  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                )\n",
       "                (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (4): ResidualAttentionBlock(\n",
       "                (attn): MultiheadAttention(\n",
       "                  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "                )\n",
       "                (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Sequential(\n",
       "                  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                  (gelu): QuickGELU()\n",
       "                  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                )\n",
       "                (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (5): ResidualAttentionBlock(\n",
       "                (attn): MultiheadAttention(\n",
       "                  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "                )\n",
       "                (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Sequential(\n",
       "                  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                  (gelu): QuickGELU()\n",
       "                  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                )\n",
       "                (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (6): ResidualAttentionBlock(\n",
       "                (attn): MultiheadAttention(\n",
       "                  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "                )\n",
       "                (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Sequential(\n",
       "                  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                  (gelu): QuickGELU()\n",
       "                  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                )\n",
       "                (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (7): ResidualAttentionBlock(\n",
       "                (attn): MultiheadAttention(\n",
       "                  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "                )\n",
       "                (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Sequential(\n",
       "                  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                  (gelu): QuickGELU()\n",
       "                  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                )\n",
       "                (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (8): ResidualAttentionBlock(\n",
       "                (attn): MultiheadAttention(\n",
       "                  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "                )\n",
       "                (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Sequential(\n",
       "                  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                  (gelu): QuickGELU()\n",
       "                  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                )\n",
       "                (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (9): ResidualAttentionBlock(\n",
       "                (attn): MultiheadAttention(\n",
       "                  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "                )\n",
       "                (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Sequential(\n",
       "                  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                  (gelu): QuickGELU()\n",
       "                  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                )\n",
       "                (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (10): ResidualAttentionBlock(\n",
       "                (attn): MultiheadAttention(\n",
       "                  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "                )\n",
       "                (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Sequential(\n",
       "                  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                  (gelu): QuickGELU()\n",
       "                  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                )\n",
       "                (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (11): ResidualAttentionBlock(\n",
       "                (attn): MultiheadAttention(\n",
       "                  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "                )\n",
       "                (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Sequential(\n",
       "                  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                  (gelu): QuickGELU()\n",
       "                  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                )\n",
       "                (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (transformer): Transformer(\n",
       "          (resblocks): Sequential(\n",
       "            (0): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (1): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (2): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (3): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (4): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (5): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (6): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (7): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (8): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (9): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (10): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (11): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (token_embedding): Embedding(49408, 512)\n",
       "        (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (features_processor): FrequencyRestorationEstimator(\n",
       "      (patch_projector): FeatureSpecificProjector(\n",
       "        (projectors): ModuleList(\n",
       "          (0-11): 12 x Projector(\n",
       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (projector): Sequential(\n",
       "              (0): Dropout(p=0.5, inplace=False)\n",
       "              (1): Linear(in_features=768, out_features=1024, bias=True)\n",
       "              (2): GELU(approximate='none')\n",
       "              (3): Dropout(p=0.5, inplace=False)\n",
       "              (4): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (5): GELU(approximate='none')\n",
       "              (6): Dropout(p=0.5, inplace=False)\n",
       "            )\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (backbone_norm): Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
       "  )\n",
       "  (attend): Softmax(dim=-1)\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       "  (to_kv): Linear(in_features=72, out_features=3072, bias=False)\n",
       "  (to_out): Sequential(\n",
       "    (0): Linear(in_features=1536, out_features=72, bias=False)\n",
       "    (1): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (norm): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
       "  (cls_head): ClassificationHead(\n",
       "    (head): Sequential(\n",
       "      (0): Linear(in_features=72, out_features=216, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.5, inplace=False)\n",
       "      (3): Linear(in_features=216, out_features=216, bias=True)\n",
       "      (4): ReLU()\n",
       "      (5): Dropout(p=0.5, inplace=False)\n",
       "      (6): Linear(in_features=216, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (semantic_mha): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=72, out_features=72, bias=True)\n",
       "  )\n",
       "  (semantic_projection): Linear(in_features=512, out_features=72, bias=True)\n",
       "  (semantic_layer_norm): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE: default config values used based on config.yaml/config.py\n",
    "batch_size = 4\n",
    "features_num = 12\n",
    "input_dim = 768\n",
    "masking_radius = 16\n",
    "\n",
    "\n",
    "features_processor = sid.FrequencyRestorationEstimator(\n",
    "    features_num=features_num,\n",
    "    input_dim=input_dim,\n",
    "    proj_dim=1024,\n",
    "    proj_layers=2,\n",
    "    patch_projection=True,\n",
    "    patch_projection_per_feature=True,\n",
    ")\n",
    "cls_head = sid.ClassificationHead(6 * features_num, 1, mlp_ratio=3)\n",
    "model = sid.PatchBasedMFViT(\n",
    "    vit=vit,\n",
    "    features_processor=features_processor,\n",
    "    cls_head=cls_head,\n",
    "    masking_radius=masking_radius,\n",
    "    img_patch_size=224,\n",
    "    img_patch_stride=224,\n",
    "    cls_vector_dim=6 * features_num,\n",
    "    num_heads=12,\n",
    "    attn_embed_dim=1536,\n",
    "    minimum_patches=1,\n",
    "    use_semantic_cross_attn_sca=\"before\",  # tested : None/before/after\n",
    "    semantic_embed_dim=512,\n",
    ")\n",
    "\n",
    "# load the model weights from weights/PatchBasedMFViT_test.pth\n",
    "model_weights_path = os.path.join(parent_dir, \"weights\", \"PatchBasedMFViT_test_05-05.pth\")\n",
    "model_weights = torch.load(model_weights_path, map_location=\"cpu\")\n",
    "model.load_state_dict(model_weights, strict=False)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d529295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([4, 3, 224, 224])\n",
      "Output shape (fixed resolution): torch.Size([4, 1])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn((batch_size, 3, 224, 224))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(x)\n",
    "\n",
    "print(f\"Output shape (fixed resolution): {out.shape}\")\n",
    "assert out.shape == (batch_size, 1), f\"Unexpected output shape: {out.shape}\"\n",
    "\n",
    "\n",
    "# # NOTE: Uncommented since this was only used for the untouched version of PatchBasedMFViT\n",
    "# # save mode to weights/PatchBasedMFViT_test.pth\n",
    "# model_path = os.path.join(\"weights\", \"PatchBasedMFViT_test_05-05.pth\")\n",
    "# os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "# torch.save(model.state_dict(), model_path)\n",
    "# print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3d0d811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patch_aggregator\n",
      "mfvit.frequencies_mask\n",
      "mfvit.vit.clip.positional_embedding\n",
      "mfvit.vit.clip.text_projection\n",
      "mfvit.vit.clip.logit_scale\n",
      "mfvit.vit.clip.visual.class_embedding\n",
      "mfvit.vit.clip.visual.positional_embedding\n",
      "mfvit.vit.clip.visual.proj\n",
      "mfvit.vit.clip.visual.conv1.weight\n",
      "mfvit.vit.clip.visual.ln_pre.weight\n",
      "mfvit.vit.clip.visual.ln_pre.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.0.attn.in_proj_weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.0.attn.in_proj_bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.0.attn.out_proj.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.0.attn.out_proj.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.0.ln_1.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.0.ln_1.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.0.mlp.c_fc.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.0.mlp.c_fc.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.0.mlp.c_proj.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.0.mlp.c_proj.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.0.ln_2.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.0.ln_2.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.1.attn.in_proj_weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.1.attn.in_proj_bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.1.attn.out_proj.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.1.attn.out_proj.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.1.ln_1.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.1.ln_1.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.1.mlp.c_fc.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.1.mlp.c_fc.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.1.mlp.c_proj.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.1.mlp.c_proj.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.1.ln_2.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.1.ln_2.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.2.attn.in_proj_weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.2.attn.in_proj_bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.2.attn.out_proj.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.2.attn.out_proj.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.2.ln_1.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.2.ln_1.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.2.mlp.c_fc.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.2.mlp.c_fc.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.2.mlp.c_proj.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.2.mlp.c_proj.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.2.ln_2.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.2.ln_2.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.3.attn.in_proj_weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.3.attn.in_proj_bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.3.attn.out_proj.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.3.attn.out_proj.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.3.ln_1.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.3.ln_1.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.3.mlp.c_fc.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.3.mlp.c_fc.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.3.mlp.c_proj.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.3.mlp.c_proj.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.3.ln_2.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.3.ln_2.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.4.attn.in_proj_weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.4.attn.in_proj_bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.4.attn.out_proj.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.4.attn.out_proj.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.4.ln_1.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.4.ln_1.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.4.mlp.c_fc.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.4.mlp.c_fc.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.4.mlp.c_proj.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.4.mlp.c_proj.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.4.ln_2.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.4.ln_2.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.5.attn.in_proj_weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.5.attn.in_proj_bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.5.attn.out_proj.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.5.attn.out_proj.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.5.ln_1.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.5.ln_1.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.5.mlp.c_fc.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.5.mlp.c_fc.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.5.mlp.c_proj.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.5.mlp.c_proj.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.5.ln_2.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.5.ln_2.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.6.attn.in_proj_weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.6.attn.in_proj_bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.6.attn.out_proj.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.6.attn.out_proj.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.6.ln_1.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.6.ln_1.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.6.mlp.c_fc.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.6.mlp.c_fc.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.6.mlp.c_proj.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.6.mlp.c_proj.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.6.ln_2.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.6.ln_2.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.7.attn.in_proj_weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.7.attn.in_proj_bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.7.attn.out_proj.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.7.attn.out_proj.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.7.ln_1.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.7.ln_1.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.7.mlp.c_fc.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.7.mlp.c_fc.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.7.mlp.c_proj.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.7.mlp.c_proj.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.7.ln_2.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.7.ln_2.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.8.attn.in_proj_weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.8.attn.in_proj_bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.8.attn.out_proj.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.8.attn.out_proj.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.8.ln_1.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.8.ln_1.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.8.mlp.c_fc.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.8.mlp.c_fc.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.8.mlp.c_proj.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.8.mlp.c_proj.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.8.ln_2.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.8.ln_2.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.9.attn.in_proj_weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.9.attn.in_proj_bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.9.attn.out_proj.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.9.attn.out_proj.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.9.ln_1.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.9.ln_1.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.9.mlp.c_fc.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.9.mlp.c_fc.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.9.mlp.c_proj.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.9.mlp.c_proj.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.9.ln_2.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.9.ln_2.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.10.attn.in_proj_weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.10.attn.in_proj_bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.10.attn.out_proj.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.10.attn.out_proj.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.10.ln_1.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.10.ln_1.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.10.mlp.c_fc.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.10.mlp.c_fc.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.10.mlp.c_proj.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.10.mlp.c_proj.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.10.ln_2.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.10.ln_2.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.11.attn.in_proj_weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.11.attn.in_proj_bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.11.attn.out_proj.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.11.attn.out_proj.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.11.ln_1.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.11.ln_1.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.11.mlp.c_fc.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.11.mlp.c_fc.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.11.mlp.c_proj.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.11.mlp.c_proj.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.11.ln_2.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.11.ln_2.bias\n",
      "mfvit.vit.clip.visual.ln_post.weight\n",
      "mfvit.vit.clip.visual.ln_post.bias\n",
      "mfvit.vit.clip.transformer.resblocks.0.attn.in_proj_weight\n",
      "mfvit.vit.clip.transformer.resblocks.0.attn.in_proj_bias\n",
      "mfvit.vit.clip.transformer.resblocks.0.attn.out_proj.weight\n",
      "mfvit.vit.clip.transformer.resblocks.0.attn.out_proj.bias\n",
      "mfvit.vit.clip.transformer.resblocks.0.ln_1.weight\n",
      "mfvit.vit.clip.transformer.resblocks.0.ln_1.bias\n",
      "mfvit.vit.clip.transformer.resblocks.0.mlp.c_fc.weight\n",
      "mfvit.vit.clip.transformer.resblocks.0.mlp.c_fc.bias\n",
      "mfvit.vit.clip.transformer.resblocks.0.mlp.c_proj.weight\n",
      "mfvit.vit.clip.transformer.resblocks.0.mlp.c_proj.bias\n",
      "mfvit.vit.clip.transformer.resblocks.0.ln_2.weight\n",
      "mfvit.vit.clip.transformer.resblocks.0.ln_2.bias\n",
      "mfvit.vit.clip.transformer.resblocks.1.attn.in_proj_weight\n",
      "mfvit.vit.clip.transformer.resblocks.1.attn.in_proj_bias\n",
      "mfvit.vit.clip.transformer.resblocks.1.attn.out_proj.weight\n",
      "mfvit.vit.clip.transformer.resblocks.1.attn.out_proj.bias\n",
      "mfvit.vit.clip.transformer.resblocks.1.ln_1.weight\n",
      "mfvit.vit.clip.transformer.resblocks.1.ln_1.bias\n",
      "mfvit.vit.clip.transformer.resblocks.1.mlp.c_fc.weight\n",
      "mfvit.vit.clip.transformer.resblocks.1.mlp.c_fc.bias\n",
      "mfvit.vit.clip.transformer.resblocks.1.mlp.c_proj.weight\n",
      "mfvit.vit.clip.transformer.resblocks.1.mlp.c_proj.bias\n",
      "mfvit.vit.clip.transformer.resblocks.1.ln_2.weight\n",
      "mfvit.vit.clip.transformer.resblocks.1.ln_2.bias\n",
      "mfvit.vit.clip.transformer.resblocks.2.attn.in_proj_weight\n",
      "mfvit.vit.clip.transformer.resblocks.2.attn.in_proj_bias\n",
      "mfvit.vit.clip.transformer.resblocks.2.attn.out_proj.weight\n",
      "mfvit.vit.clip.transformer.resblocks.2.attn.out_proj.bias\n",
      "mfvit.vit.clip.transformer.resblocks.2.ln_1.weight\n",
      "mfvit.vit.clip.transformer.resblocks.2.ln_1.bias\n",
      "mfvit.vit.clip.transformer.resblocks.2.mlp.c_fc.weight\n",
      "mfvit.vit.clip.transformer.resblocks.2.mlp.c_fc.bias\n",
      "mfvit.vit.clip.transformer.resblocks.2.mlp.c_proj.weight\n",
      "mfvit.vit.clip.transformer.resblocks.2.mlp.c_proj.bias\n",
      "mfvit.vit.clip.transformer.resblocks.2.ln_2.weight\n",
      "mfvit.vit.clip.transformer.resblocks.2.ln_2.bias\n",
      "mfvit.vit.clip.transformer.resblocks.3.attn.in_proj_weight\n",
      "mfvit.vit.clip.transformer.resblocks.3.attn.in_proj_bias\n",
      "mfvit.vit.clip.transformer.resblocks.3.attn.out_proj.weight\n",
      "mfvit.vit.clip.transformer.resblocks.3.attn.out_proj.bias\n",
      "mfvit.vit.clip.transformer.resblocks.3.ln_1.weight\n",
      "mfvit.vit.clip.transformer.resblocks.3.ln_1.bias\n",
      "mfvit.vit.clip.transformer.resblocks.3.mlp.c_fc.weight\n",
      "mfvit.vit.clip.transformer.resblocks.3.mlp.c_fc.bias\n",
      "mfvit.vit.clip.transformer.resblocks.3.mlp.c_proj.weight\n",
      "mfvit.vit.clip.transformer.resblocks.3.mlp.c_proj.bias\n",
      "mfvit.vit.clip.transformer.resblocks.3.ln_2.weight\n",
      "mfvit.vit.clip.transformer.resblocks.3.ln_2.bias\n",
      "mfvit.vit.clip.transformer.resblocks.4.attn.in_proj_weight\n",
      "mfvit.vit.clip.transformer.resblocks.4.attn.in_proj_bias\n",
      "mfvit.vit.clip.transformer.resblocks.4.attn.out_proj.weight\n",
      "mfvit.vit.clip.transformer.resblocks.4.attn.out_proj.bias\n",
      "mfvit.vit.clip.transformer.resblocks.4.ln_1.weight\n",
      "mfvit.vit.clip.transformer.resblocks.4.ln_1.bias\n",
      "mfvit.vit.clip.transformer.resblocks.4.mlp.c_fc.weight\n",
      "mfvit.vit.clip.transformer.resblocks.4.mlp.c_fc.bias\n",
      "mfvit.vit.clip.transformer.resblocks.4.mlp.c_proj.weight\n",
      "mfvit.vit.clip.transformer.resblocks.4.mlp.c_proj.bias\n",
      "mfvit.vit.clip.transformer.resblocks.4.ln_2.weight\n",
      "mfvit.vit.clip.transformer.resblocks.4.ln_2.bias\n",
      "mfvit.vit.clip.transformer.resblocks.5.attn.in_proj_weight\n",
      "mfvit.vit.clip.transformer.resblocks.5.attn.in_proj_bias\n",
      "mfvit.vit.clip.transformer.resblocks.5.attn.out_proj.weight\n",
      "mfvit.vit.clip.transformer.resblocks.5.attn.out_proj.bias\n",
      "mfvit.vit.clip.transformer.resblocks.5.ln_1.weight\n",
      "mfvit.vit.clip.transformer.resblocks.5.ln_1.bias\n",
      "mfvit.vit.clip.transformer.resblocks.5.mlp.c_fc.weight\n",
      "mfvit.vit.clip.transformer.resblocks.5.mlp.c_fc.bias\n",
      "mfvit.vit.clip.transformer.resblocks.5.mlp.c_proj.weight\n",
      "mfvit.vit.clip.transformer.resblocks.5.mlp.c_proj.bias\n",
      "mfvit.vit.clip.transformer.resblocks.5.ln_2.weight\n",
      "mfvit.vit.clip.transformer.resblocks.5.ln_2.bias\n",
      "mfvit.vit.clip.transformer.resblocks.6.attn.in_proj_weight\n",
      "mfvit.vit.clip.transformer.resblocks.6.attn.in_proj_bias\n",
      "mfvit.vit.clip.transformer.resblocks.6.attn.out_proj.weight\n",
      "mfvit.vit.clip.transformer.resblocks.6.attn.out_proj.bias\n",
      "mfvit.vit.clip.transformer.resblocks.6.ln_1.weight\n",
      "mfvit.vit.clip.transformer.resblocks.6.ln_1.bias\n",
      "mfvit.vit.clip.transformer.resblocks.6.mlp.c_fc.weight\n",
      "mfvit.vit.clip.transformer.resblocks.6.mlp.c_fc.bias\n",
      "mfvit.vit.clip.transformer.resblocks.6.mlp.c_proj.weight\n",
      "mfvit.vit.clip.transformer.resblocks.6.mlp.c_proj.bias\n",
      "mfvit.vit.clip.transformer.resblocks.6.ln_2.weight\n",
      "mfvit.vit.clip.transformer.resblocks.6.ln_2.bias\n",
      "mfvit.vit.clip.transformer.resblocks.7.attn.in_proj_weight\n",
      "mfvit.vit.clip.transformer.resblocks.7.attn.in_proj_bias\n",
      "mfvit.vit.clip.transformer.resblocks.7.attn.out_proj.weight\n",
      "mfvit.vit.clip.transformer.resblocks.7.attn.out_proj.bias\n",
      "mfvit.vit.clip.transformer.resblocks.7.ln_1.weight\n",
      "mfvit.vit.clip.transformer.resblocks.7.ln_1.bias\n",
      "mfvit.vit.clip.transformer.resblocks.7.mlp.c_fc.weight\n",
      "mfvit.vit.clip.transformer.resblocks.7.mlp.c_fc.bias\n",
      "mfvit.vit.clip.transformer.resblocks.7.mlp.c_proj.weight\n",
      "mfvit.vit.clip.transformer.resblocks.7.mlp.c_proj.bias\n",
      "mfvit.vit.clip.transformer.resblocks.7.ln_2.weight\n",
      "mfvit.vit.clip.transformer.resblocks.7.ln_2.bias\n",
      "mfvit.vit.clip.transformer.resblocks.8.attn.in_proj_weight\n",
      "mfvit.vit.clip.transformer.resblocks.8.attn.in_proj_bias\n",
      "mfvit.vit.clip.transformer.resblocks.8.attn.out_proj.weight\n",
      "mfvit.vit.clip.transformer.resblocks.8.attn.out_proj.bias\n",
      "mfvit.vit.clip.transformer.resblocks.8.ln_1.weight\n",
      "mfvit.vit.clip.transformer.resblocks.8.ln_1.bias\n",
      "mfvit.vit.clip.transformer.resblocks.8.mlp.c_fc.weight\n",
      "mfvit.vit.clip.transformer.resblocks.8.mlp.c_fc.bias\n",
      "mfvit.vit.clip.transformer.resblocks.8.mlp.c_proj.weight\n",
      "mfvit.vit.clip.transformer.resblocks.8.mlp.c_proj.bias\n",
      "mfvit.vit.clip.transformer.resblocks.8.ln_2.weight\n",
      "mfvit.vit.clip.transformer.resblocks.8.ln_2.bias\n",
      "mfvit.vit.clip.transformer.resblocks.9.attn.in_proj_weight\n",
      "mfvit.vit.clip.transformer.resblocks.9.attn.in_proj_bias\n",
      "mfvit.vit.clip.transformer.resblocks.9.attn.out_proj.weight\n",
      "mfvit.vit.clip.transformer.resblocks.9.attn.out_proj.bias\n",
      "mfvit.vit.clip.transformer.resblocks.9.ln_1.weight\n",
      "mfvit.vit.clip.transformer.resblocks.9.ln_1.bias\n",
      "mfvit.vit.clip.transformer.resblocks.9.mlp.c_fc.weight\n",
      "mfvit.vit.clip.transformer.resblocks.9.mlp.c_fc.bias\n",
      "mfvit.vit.clip.transformer.resblocks.9.mlp.c_proj.weight\n",
      "mfvit.vit.clip.transformer.resblocks.9.mlp.c_proj.bias\n",
      "mfvit.vit.clip.transformer.resblocks.9.ln_2.weight\n",
      "mfvit.vit.clip.transformer.resblocks.9.ln_2.bias\n",
      "mfvit.vit.clip.transformer.resblocks.10.attn.in_proj_weight\n",
      "mfvit.vit.clip.transformer.resblocks.10.attn.in_proj_bias\n",
      "mfvit.vit.clip.transformer.resblocks.10.attn.out_proj.weight\n",
      "mfvit.vit.clip.transformer.resblocks.10.attn.out_proj.bias\n",
      "mfvit.vit.clip.transformer.resblocks.10.ln_1.weight\n",
      "mfvit.vit.clip.transformer.resblocks.10.ln_1.bias\n",
      "mfvit.vit.clip.transformer.resblocks.10.mlp.c_fc.weight\n",
      "mfvit.vit.clip.transformer.resblocks.10.mlp.c_fc.bias\n",
      "mfvit.vit.clip.transformer.resblocks.10.mlp.c_proj.weight\n",
      "mfvit.vit.clip.transformer.resblocks.10.mlp.c_proj.bias\n",
      "mfvit.vit.clip.transformer.resblocks.10.ln_2.weight\n",
      "mfvit.vit.clip.transformer.resblocks.10.ln_2.bias\n",
      "mfvit.vit.clip.transformer.resblocks.11.attn.in_proj_weight\n",
      "mfvit.vit.clip.transformer.resblocks.11.attn.in_proj_bias\n",
      "mfvit.vit.clip.transformer.resblocks.11.attn.out_proj.weight\n",
      "mfvit.vit.clip.transformer.resblocks.11.attn.out_proj.bias\n",
      "mfvit.vit.clip.transformer.resblocks.11.ln_1.weight\n",
      "mfvit.vit.clip.transformer.resblocks.11.ln_1.bias\n",
      "mfvit.vit.clip.transformer.resblocks.11.mlp.c_fc.weight\n",
      "mfvit.vit.clip.transformer.resblocks.11.mlp.c_fc.bias\n",
      "mfvit.vit.clip.transformer.resblocks.11.mlp.c_proj.weight\n",
      "mfvit.vit.clip.transformer.resblocks.11.mlp.c_proj.bias\n",
      "mfvit.vit.clip.transformer.resblocks.11.ln_2.weight\n",
      "mfvit.vit.clip.transformer.resblocks.11.ln_2.bias\n",
      "mfvit.vit.clip.token_embedding.weight\n",
      "mfvit.vit.clip.ln_final.weight\n",
      "mfvit.vit.clip.ln_final.bias\n",
      "mfvit.features_processor.patch_projector.projectors.0.norm1.weight\n",
      "mfvit.features_processor.patch_projector.projectors.0.norm1.bias\n",
      "mfvit.features_processor.patch_projector.projectors.0.projector.1.weight\n",
      "mfvit.features_processor.patch_projector.projectors.0.projector.1.bias\n",
      "mfvit.features_processor.patch_projector.projectors.0.projector.4.weight\n",
      "mfvit.features_processor.patch_projector.projectors.0.projector.4.bias\n",
      "mfvit.features_processor.patch_projector.projectors.0.norm2.weight\n",
      "mfvit.features_processor.patch_projector.projectors.0.norm2.bias\n",
      "mfvit.features_processor.patch_projector.projectors.1.norm1.weight\n",
      "mfvit.features_processor.patch_projector.projectors.1.norm1.bias\n",
      "mfvit.features_processor.patch_projector.projectors.1.projector.1.weight\n",
      "mfvit.features_processor.patch_projector.projectors.1.projector.1.bias\n",
      "mfvit.features_processor.patch_projector.projectors.1.projector.4.weight\n",
      "mfvit.features_processor.patch_projector.projectors.1.projector.4.bias\n",
      "mfvit.features_processor.patch_projector.projectors.1.norm2.weight\n",
      "mfvit.features_processor.patch_projector.projectors.1.norm2.bias\n",
      "mfvit.features_processor.patch_projector.projectors.2.norm1.weight\n",
      "mfvit.features_processor.patch_projector.projectors.2.norm1.bias\n",
      "mfvit.features_processor.patch_projector.projectors.2.projector.1.weight\n",
      "mfvit.features_processor.patch_projector.projectors.2.projector.1.bias\n",
      "mfvit.features_processor.patch_projector.projectors.2.projector.4.weight\n",
      "mfvit.features_processor.patch_projector.projectors.2.projector.4.bias\n",
      "mfvit.features_processor.patch_projector.projectors.2.norm2.weight\n",
      "mfvit.features_processor.patch_projector.projectors.2.norm2.bias\n",
      "mfvit.features_processor.patch_projector.projectors.3.norm1.weight\n",
      "mfvit.features_processor.patch_projector.projectors.3.norm1.bias\n",
      "mfvit.features_processor.patch_projector.projectors.3.projector.1.weight\n",
      "mfvit.features_processor.patch_projector.projectors.3.projector.1.bias\n",
      "mfvit.features_processor.patch_projector.projectors.3.projector.4.weight\n",
      "mfvit.features_processor.patch_projector.projectors.3.projector.4.bias\n",
      "mfvit.features_processor.patch_projector.projectors.3.norm2.weight\n",
      "mfvit.features_processor.patch_projector.projectors.3.norm2.bias\n",
      "mfvit.features_processor.patch_projector.projectors.4.norm1.weight\n",
      "mfvit.features_processor.patch_projector.projectors.4.norm1.bias\n",
      "mfvit.features_processor.patch_projector.projectors.4.projector.1.weight\n",
      "mfvit.features_processor.patch_projector.projectors.4.projector.1.bias\n",
      "mfvit.features_processor.patch_projector.projectors.4.projector.4.weight\n",
      "mfvit.features_processor.patch_projector.projectors.4.projector.4.bias\n",
      "mfvit.features_processor.patch_projector.projectors.4.norm2.weight\n",
      "mfvit.features_processor.patch_projector.projectors.4.norm2.bias\n",
      "mfvit.features_processor.patch_projector.projectors.5.norm1.weight\n",
      "mfvit.features_processor.patch_projector.projectors.5.norm1.bias\n",
      "mfvit.features_processor.patch_projector.projectors.5.projector.1.weight\n",
      "mfvit.features_processor.patch_projector.projectors.5.projector.1.bias\n",
      "mfvit.features_processor.patch_projector.projectors.5.projector.4.weight\n",
      "mfvit.features_processor.patch_projector.projectors.5.projector.4.bias\n",
      "mfvit.features_processor.patch_projector.projectors.5.norm2.weight\n",
      "mfvit.features_processor.patch_projector.projectors.5.norm2.bias\n",
      "mfvit.features_processor.patch_projector.projectors.6.norm1.weight\n",
      "mfvit.features_processor.patch_projector.projectors.6.norm1.bias\n",
      "mfvit.features_processor.patch_projector.projectors.6.projector.1.weight\n",
      "mfvit.features_processor.patch_projector.projectors.6.projector.1.bias\n",
      "mfvit.features_processor.patch_projector.projectors.6.projector.4.weight\n",
      "mfvit.features_processor.patch_projector.projectors.6.projector.4.bias\n",
      "mfvit.features_processor.patch_projector.projectors.6.norm2.weight\n",
      "mfvit.features_processor.patch_projector.projectors.6.norm2.bias\n",
      "mfvit.features_processor.patch_projector.projectors.7.norm1.weight\n",
      "mfvit.features_processor.patch_projector.projectors.7.norm1.bias\n",
      "mfvit.features_processor.patch_projector.projectors.7.projector.1.weight\n",
      "mfvit.features_processor.patch_projector.projectors.7.projector.1.bias\n",
      "mfvit.features_processor.patch_projector.projectors.7.projector.4.weight\n",
      "mfvit.features_processor.patch_projector.projectors.7.projector.4.bias\n",
      "mfvit.features_processor.patch_projector.projectors.7.norm2.weight\n",
      "mfvit.features_processor.patch_projector.projectors.7.norm2.bias\n",
      "mfvit.features_processor.patch_projector.projectors.8.norm1.weight\n",
      "mfvit.features_processor.patch_projector.projectors.8.norm1.bias\n",
      "mfvit.features_processor.patch_projector.projectors.8.projector.1.weight\n",
      "mfvit.features_processor.patch_projector.projectors.8.projector.1.bias\n",
      "mfvit.features_processor.patch_projector.projectors.8.projector.4.weight\n",
      "mfvit.features_processor.patch_projector.projectors.8.projector.4.bias\n",
      "mfvit.features_processor.patch_projector.projectors.8.norm2.weight\n",
      "mfvit.features_processor.patch_projector.projectors.8.norm2.bias\n",
      "mfvit.features_processor.patch_projector.projectors.9.norm1.weight\n",
      "mfvit.features_processor.patch_projector.projectors.9.norm1.bias\n",
      "mfvit.features_processor.patch_projector.projectors.9.projector.1.weight\n",
      "mfvit.features_processor.patch_projector.projectors.9.projector.1.bias\n",
      "mfvit.features_processor.patch_projector.projectors.9.projector.4.weight\n",
      "mfvit.features_processor.patch_projector.projectors.9.projector.4.bias\n",
      "mfvit.features_processor.patch_projector.projectors.9.norm2.weight\n",
      "mfvit.features_processor.patch_projector.projectors.9.norm2.bias\n",
      "mfvit.features_processor.patch_projector.projectors.10.norm1.weight\n",
      "mfvit.features_processor.patch_projector.projectors.10.norm1.bias\n",
      "mfvit.features_processor.patch_projector.projectors.10.projector.1.weight\n",
      "mfvit.features_processor.patch_projector.projectors.10.projector.1.bias\n",
      "mfvit.features_processor.patch_projector.projectors.10.projector.4.weight\n",
      "mfvit.features_processor.patch_projector.projectors.10.projector.4.bias\n",
      "mfvit.features_processor.patch_projector.projectors.10.norm2.weight\n",
      "mfvit.features_processor.patch_projector.projectors.10.norm2.bias\n",
      "mfvit.features_processor.patch_projector.projectors.11.norm1.weight\n",
      "mfvit.features_processor.patch_projector.projectors.11.norm1.bias\n",
      "mfvit.features_processor.patch_projector.projectors.11.projector.1.weight\n",
      "mfvit.features_processor.patch_projector.projectors.11.projector.1.bias\n",
      "mfvit.features_processor.patch_projector.projectors.11.projector.4.weight\n",
      "mfvit.features_processor.patch_projector.projectors.11.projector.4.bias\n",
      "mfvit.features_processor.patch_projector.projectors.11.norm2.weight\n",
      "mfvit.features_processor.patch_projector.projectors.11.norm2.bias\n",
      "to_kv.weight\n",
      "to_out.0.weight\n",
      "norm.weight\n",
      "norm.bias\n",
      "cls_head.head.0.weight\n",
      "cls_head.head.0.bias\n",
      "cls_head.head.3.weight\n",
      "cls_head.head.3.bias\n",
      "cls_head.head.6.weight\n",
      "cls_head.head.6.bias\n"
     ]
    }
   ],
   "source": [
    "# OLD KEYS: untouched version of PatchBasedMFViT; no semantic cross-attention parameters\n",
    "for key in model_weights.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5cc64e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patch_aggregator\n",
      "mfvit.frequencies_mask\n",
      "mfvit.vit.clip.positional_embedding\n",
      "mfvit.vit.clip.text_projection\n",
      "mfvit.vit.clip.logit_scale\n",
      "mfvit.vit.clip.visual.class_embedding\n",
      "mfvit.vit.clip.visual.positional_embedding\n",
      "mfvit.vit.clip.visual.proj\n",
      "mfvit.vit.clip.visual.conv1.weight\n",
      "mfvit.vit.clip.visual.ln_pre.weight\n",
      "mfvit.vit.clip.visual.ln_pre.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.0.attn.in_proj_weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.0.attn.in_proj_bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.0.attn.out_proj.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.0.attn.out_proj.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.0.ln_1.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.0.ln_1.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.0.mlp.c_fc.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.0.mlp.c_fc.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.0.mlp.c_proj.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.0.mlp.c_proj.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.0.ln_2.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.0.ln_2.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.1.attn.in_proj_weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.1.attn.in_proj_bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.1.attn.out_proj.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.1.attn.out_proj.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.1.ln_1.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.1.ln_1.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.1.mlp.c_fc.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.1.mlp.c_fc.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.1.mlp.c_proj.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.1.mlp.c_proj.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.1.ln_2.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.1.ln_2.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.2.attn.in_proj_weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.2.attn.in_proj_bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.2.attn.out_proj.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.2.attn.out_proj.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.2.ln_1.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.2.ln_1.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.2.mlp.c_fc.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.2.mlp.c_fc.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.2.mlp.c_proj.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.2.mlp.c_proj.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.2.ln_2.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.2.ln_2.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.3.attn.in_proj_weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.3.attn.in_proj_bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.3.attn.out_proj.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.3.attn.out_proj.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.3.ln_1.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.3.ln_1.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.3.mlp.c_fc.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.3.mlp.c_fc.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.3.mlp.c_proj.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.3.mlp.c_proj.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.3.ln_2.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.3.ln_2.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.4.attn.in_proj_weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.4.attn.in_proj_bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.4.attn.out_proj.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.4.attn.out_proj.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.4.ln_1.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.4.ln_1.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.4.mlp.c_fc.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.4.mlp.c_fc.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.4.mlp.c_proj.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.4.mlp.c_proj.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.4.ln_2.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.4.ln_2.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.5.attn.in_proj_weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.5.attn.in_proj_bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.5.attn.out_proj.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.5.attn.out_proj.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.5.ln_1.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.5.ln_1.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.5.mlp.c_fc.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.5.mlp.c_fc.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.5.mlp.c_proj.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.5.mlp.c_proj.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.5.ln_2.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.5.ln_2.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.6.attn.in_proj_weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.6.attn.in_proj_bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.6.attn.out_proj.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.6.attn.out_proj.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.6.ln_1.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.6.ln_1.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.6.mlp.c_fc.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.6.mlp.c_fc.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.6.mlp.c_proj.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.6.mlp.c_proj.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.6.ln_2.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.6.ln_2.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.7.attn.in_proj_weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.7.attn.in_proj_bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.7.attn.out_proj.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.7.attn.out_proj.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.7.ln_1.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.7.ln_1.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.7.mlp.c_fc.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.7.mlp.c_fc.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.7.mlp.c_proj.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.7.mlp.c_proj.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.7.ln_2.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.7.ln_2.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.8.attn.in_proj_weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.8.attn.in_proj_bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.8.attn.out_proj.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.8.attn.out_proj.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.8.ln_1.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.8.ln_1.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.8.mlp.c_fc.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.8.mlp.c_fc.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.8.mlp.c_proj.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.8.mlp.c_proj.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.8.ln_2.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.8.ln_2.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.9.attn.in_proj_weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.9.attn.in_proj_bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.9.attn.out_proj.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.9.attn.out_proj.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.9.ln_1.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.9.ln_1.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.9.mlp.c_fc.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.9.mlp.c_fc.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.9.mlp.c_proj.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.9.mlp.c_proj.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.9.ln_2.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.9.ln_2.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.10.attn.in_proj_weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.10.attn.in_proj_bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.10.attn.out_proj.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.10.attn.out_proj.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.10.ln_1.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.10.ln_1.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.10.mlp.c_fc.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.10.mlp.c_fc.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.10.mlp.c_proj.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.10.mlp.c_proj.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.10.ln_2.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.10.ln_2.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.11.attn.in_proj_weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.11.attn.in_proj_bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.11.attn.out_proj.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.11.attn.out_proj.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.11.ln_1.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.11.ln_1.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.11.mlp.c_fc.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.11.mlp.c_fc.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.11.mlp.c_proj.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.11.mlp.c_proj.bias\n",
      "mfvit.vit.clip.visual.transformer.resblocks.11.ln_2.weight\n",
      "mfvit.vit.clip.visual.transformer.resblocks.11.ln_2.bias\n",
      "mfvit.vit.clip.visual.ln_post.weight\n",
      "mfvit.vit.clip.visual.ln_post.bias\n",
      "mfvit.vit.clip.transformer.resblocks.0.attn.in_proj_weight\n",
      "mfvit.vit.clip.transformer.resblocks.0.attn.in_proj_bias\n",
      "mfvit.vit.clip.transformer.resblocks.0.attn.out_proj.weight\n",
      "mfvit.vit.clip.transformer.resblocks.0.attn.out_proj.bias\n",
      "mfvit.vit.clip.transformer.resblocks.0.ln_1.weight\n",
      "mfvit.vit.clip.transformer.resblocks.0.ln_1.bias\n",
      "mfvit.vit.clip.transformer.resblocks.0.mlp.c_fc.weight\n",
      "mfvit.vit.clip.transformer.resblocks.0.mlp.c_fc.bias\n",
      "mfvit.vit.clip.transformer.resblocks.0.mlp.c_proj.weight\n",
      "mfvit.vit.clip.transformer.resblocks.0.mlp.c_proj.bias\n",
      "mfvit.vit.clip.transformer.resblocks.0.ln_2.weight\n",
      "mfvit.vit.clip.transformer.resblocks.0.ln_2.bias\n",
      "mfvit.vit.clip.transformer.resblocks.1.attn.in_proj_weight\n",
      "mfvit.vit.clip.transformer.resblocks.1.attn.in_proj_bias\n",
      "mfvit.vit.clip.transformer.resblocks.1.attn.out_proj.weight\n",
      "mfvit.vit.clip.transformer.resblocks.1.attn.out_proj.bias\n",
      "mfvit.vit.clip.transformer.resblocks.1.ln_1.weight\n",
      "mfvit.vit.clip.transformer.resblocks.1.ln_1.bias\n",
      "mfvit.vit.clip.transformer.resblocks.1.mlp.c_fc.weight\n",
      "mfvit.vit.clip.transformer.resblocks.1.mlp.c_fc.bias\n",
      "mfvit.vit.clip.transformer.resblocks.1.mlp.c_proj.weight\n",
      "mfvit.vit.clip.transformer.resblocks.1.mlp.c_proj.bias\n",
      "mfvit.vit.clip.transformer.resblocks.1.ln_2.weight\n",
      "mfvit.vit.clip.transformer.resblocks.1.ln_2.bias\n",
      "mfvit.vit.clip.transformer.resblocks.2.attn.in_proj_weight\n",
      "mfvit.vit.clip.transformer.resblocks.2.attn.in_proj_bias\n",
      "mfvit.vit.clip.transformer.resblocks.2.attn.out_proj.weight\n",
      "mfvit.vit.clip.transformer.resblocks.2.attn.out_proj.bias\n",
      "mfvit.vit.clip.transformer.resblocks.2.ln_1.weight\n",
      "mfvit.vit.clip.transformer.resblocks.2.ln_1.bias\n",
      "mfvit.vit.clip.transformer.resblocks.2.mlp.c_fc.weight\n",
      "mfvit.vit.clip.transformer.resblocks.2.mlp.c_fc.bias\n",
      "mfvit.vit.clip.transformer.resblocks.2.mlp.c_proj.weight\n",
      "mfvit.vit.clip.transformer.resblocks.2.mlp.c_proj.bias\n",
      "mfvit.vit.clip.transformer.resblocks.2.ln_2.weight\n",
      "mfvit.vit.clip.transformer.resblocks.2.ln_2.bias\n",
      "mfvit.vit.clip.transformer.resblocks.3.attn.in_proj_weight\n",
      "mfvit.vit.clip.transformer.resblocks.3.attn.in_proj_bias\n",
      "mfvit.vit.clip.transformer.resblocks.3.attn.out_proj.weight\n",
      "mfvit.vit.clip.transformer.resblocks.3.attn.out_proj.bias\n",
      "mfvit.vit.clip.transformer.resblocks.3.ln_1.weight\n",
      "mfvit.vit.clip.transformer.resblocks.3.ln_1.bias\n",
      "mfvit.vit.clip.transformer.resblocks.3.mlp.c_fc.weight\n",
      "mfvit.vit.clip.transformer.resblocks.3.mlp.c_fc.bias\n",
      "mfvit.vit.clip.transformer.resblocks.3.mlp.c_proj.weight\n",
      "mfvit.vit.clip.transformer.resblocks.3.mlp.c_proj.bias\n",
      "mfvit.vit.clip.transformer.resblocks.3.ln_2.weight\n",
      "mfvit.vit.clip.transformer.resblocks.3.ln_2.bias\n",
      "mfvit.vit.clip.transformer.resblocks.4.attn.in_proj_weight\n",
      "mfvit.vit.clip.transformer.resblocks.4.attn.in_proj_bias\n",
      "mfvit.vit.clip.transformer.resblocks.4.attn.out_proj.weight\n",
      "mfvit.vit.clip.transformer.resblocks.4.attn.out_proj.bias\n",
      "mfvit.vit.clip.transformer.resblocks.4.ln_1.weight\n",
      "mfvit.vit.clip.transformer.resblocks.4.ln_1.bias\n",
      "mfvit.vit.clip.transformer.resblocks.4.mlp.c_fc.weight\n",
      "mfvit.vit.clip.transformer.resblocks.4.mlp.c_fc.bias\n",
      "mfvit.vit.clip.transformer.resblocks.4.mlp.c_proj.weight\n",
      "mfvit.vit.clip.transformer.resblocks.4.mlp.c_proj.bias\n",
      "mfvit.vit.clip.transformer.resblocks.4.ln_2.weight\n",
      "mfvit.vit.clip.transformer.resblocks.4.ln_2.bias\n",
      "mfvit.vit.clip.transformer.resblocks.5.attn.in_proj_weight\n",
      "mfvit.vit.clip.transformer.resblocks.5.attn.in_proj_bias\n",
      "mfvit.vit.clip.transformer.resblocks.5.attn.out_proj.weight\n",
      "mfvit.vit.clip.transformer.resblocks.5.attn.out_proj.bias\n",
      "mfvit.vit.clip.transformer.resblocks.5.ln_1.weight\n",
      "mfvit.vit.clip.transformer.resblocks.5.ln_1.bias\n",
      "mfvit.vit.clip.transformer.resblocks.5.mlp.c_fc.weight\n",
      "mfvit.vit.clip.transformer.resblocks.5.mlp.c_fc.bias\n",
      "mfvit.vit.clip.transformer.resblocks.5.mlp.c_proj.weight\n",
      "mfvit.vit.clip.transformer.resblocks.5.mlp.c_proj.bias\n",
      "mfvit.vit.clip.transformer.resblocks.5.ln_2.weight\n",
      "mfvit.vit.clip.transformer.resblocks.5.ln_2.bias\n",
      "mfvit.vit.clip.transformer.resblocks.6.attn.in_proj_weight\n",
      "mfvit.vit.clip.transformer.resblocks.6.attn.in_proj_bias\n",
      "mfvit.vit.clip.transformer.resblocks.6.attn.out_proj.weight\n",
      "mfvit.vit.clip.transformer.resblocks.6.attn.out_proj.bias\n",
      "mfvit.vit.clip.transformer.resblocks.6.ln_1.weight\n",
      "mfvit.vit.clip.transformer.resblocks.6.ln_1.bias\n",
      "mfvit.vit.clip.transformer.resblocks.6.mlp.c_fc.weight\n",
      "mfvit.vit.clip.transformer.resblocks.6.mlp.c_fc.bias\n",
      "mfvit.vit.clip.transformer.resblocks.6.mlp.c_proj.weight\n",
      "mfvit.vit.clip.transformer.resblocks.6.mlp.c_proj.bias\n",
      "mfvit.vit.clip.transformer.resblocks.6.ln_2.weight\n",
      "mfvit.vit.clip.transformer.resblocks.6.ln_2.bias\n",
      "mfvit.vit.clip.transformer.resblocks.7.attn.in_proj_weight\n",
      "mfvit.vit.clip.transformer.resblocks.7.attn.in_proj_bias\n",
      "mfvit.vit.clip.transformer.resblocks.7.attn.out_proj.weight\n",
      "mfvit.vit.clip.transformer.resblocks.7.attn.out_proj.bias\n",
      "mfvit.vit.clip.transformer.resblocks.7.ln_1.weight\n",
      "mfvit.vit.clip.transformer.resblocks.7.ln_1.bias\n",
      "mfvit.vit.clip.transformer.resblocks.7.mlp.c_fc.weight\n",
      "mfvit.vit.clip.transformer.resblocks.7.mlp.c_fc.bias\n",
      "mfvit.vit.clip.transformer.resblocks.7.mlp.c_proj.weight\n",
      "mfvit.vit.clip.transformer.resblocks.7.mlp.c_proj.bias\n",
      "mfvit.vit.clip.transformer.resblocks.7.ln_2.weight\n",
      "mfvit.vit.clip.transformer.resblocks.7.ln_2.bias\n",
      "mfvit.vit.clip.transformer.resblocks.8.attn.in_proj_weight\n",
      "mfvit.vit.clip.transformer.resblocks.8.attn.in_proj_bias\n",
      "mfvit.vit.clip.transformer.resblocks.8.attn.out_proj.weight\n",
      "mfvit.vit.clip.transformer.resblocks.8.attn.out_proj.bias\n",
      "mfvit.vit.clip.transformer.resblocks.8.ln_1.weight\n",
      "mfvit.vit.clip.transformer.resblocks.8.ln_1.bias\n",
      "mfvit.vit.clip.transformer.resblocks.8.mlp.c_fc.weight\n",
      "mfvit.vit.clip.transformer.resblocks.8.mlp.c_fc.bias\n",
      "mfvit.vit.clip.transformer.resblocks.8.mlp.c_proj.weight\n",
      "mfvit.vit.clip.transformer.resblocks.8.mlp.c_proj.bias\n",
      "mfvit.vit.clip.transformer.resblocks.8.ln_2.weight\n",
      "mfvit.vit.clip.transformer.resblocks.8.ln_2.bias\n",
      "mfvit.vit.clip.transformer.resblocks.9.attn.in_proj_weight\n",
      "mfvit.vit.clip.transformer.resblocks.9.attn.in_proj_bias\n",
      "mfvit.vit.clip.transformer.resblocks.9.attn.out_proj.weight\n",
      "mfvit.vit.clip.transformer.resblocks.9.attn.out_proj.bias\n",
      "mfvit.vit.clip.transformer.resblocks.9.ln_1.weight\n",
      "mfvit.vit.clip.transformer.resblocks.9.ln_1.bias\n",
      "mfvit.vit.clip.transformer.resblocks.9.mlp.c_fc.weight\n",
      "mfvit.vit.clip.transformer.resblocks.9.mlp.c_fc.bias\n",
      "mfvit.vit.clip.transformer.resblocks.9.mlp.c_proj.weight\n",
      "mfvit.vit.clip.transformer.resblocks.9.mlp.c_proj.bias\n",
      "mfvit.vit.clip.transformer.resblocks.9.ln_2.weight\n",
      "mfvit.vit.clip.transformer.resblocks.9.ln_2.bias\n",
      "mfvit.vit.clip.transformer.resblocks.10.attn.in_proj_weight\n",
      "mfvit.vit.clip.transformer.resblocks.10.attn.in_proj_bias\n",
      "mfvit.vit.clip.transformer.resblocks.10.attn.out_proj.weight\n",
      "mfvit.vit.clip.transformer.resblocks.10.attn.out_proj.bias\n",
      "mfvit.vit.clip.transformer.resblocks.10.ln_1.weight\n",
      "mfvit.vit.clip.transformer.resblocks.10.ln_1.bias\n",
      "mfvit.vit.clip.transformer.resblocks.10.mlp.c_fc.weight\n",
      "mfvit.vit.clip.transformer.resblocks.10.mlp.c_fc.bias\n",
      "mfvit.vit.clip.transformer.resblocks.10.mlp.c_proj.weight\n",
      "mfvit.vit.clip.transformer.resblocks.10.mlp.c_proj.bias\n",
      "mfvit.vit.clip.transformer.resblocks.10.ln_2.weight\n",
      "mfvit.vit.clip.transformer.resblocks.10.ln_2.bias\n",
      "mfvit.vit.clip.transformer.resblocks.11.attn.in_proj_weight\n",
      "mfvit.vit.clip.transformer.resblocks.11.attn.in_proj_bias\n",
      "mfvit.vit.clip.transformer.resblocks.11.attn.out_proj.weight\n",
      "mfvit.vit.clip.transformer.resblocks.11.attn.out_proj.bias\n",
      "mfvit.vit.clip.transformer.resblocks.11.ln_1.weight\n",
      "mfvit.vit.clip.transformer.resblocks.11.ln_1.bias\n",
      "mfvit.vit.clip.transformer.resblocks.11.mlp.c_fc.weight\n",
      "mfvit.vit.clip.transformer.resblocks.11.mlp.c_fc.bias\n",
      "mfvit.vit.clip.transformer.resblocks.11.mlp.c_proj.weight\n",
      "mfvit.vit.clip.transformer.resblocks.11.mlp.c_proj.bias\n",
      "mfvit.vit.clip.transformer.resblocks.11.ln_2.weight\n",
      "mfvit.vit.clip.transformer.resblocks.11.ln_2.bias\n",
      "mfvit.vit.clip.token_embedding.weight\n",
      "mfvit.vit.clip.ln_final.weight\n",
      "mfvit.vit.clip.ln_final.bias\n",
      "mfvit.features_processor.patch_projector.projectors.0.norm1.weight\n",
      "mfvit.features_processor.patch_projector.projectors.0.norm1.bias\n",
      "mfvit.features_processor.patch_projector.projectors.0.projector.1.weight\n",
      "mfvit.features_processor.patch_projector.projectors.0.projector.1.bias\n",
      "mfvit.features_processor.patch_projector.projectors.0.projector.4.weight\n",
      "mfvit.features_processor.patch_projector.projectors.0.projector.4.bias\n",
      "mfvit.features_processor.patch_projector.projectors.0.norm2.weight\n",
      "mfvit.features_processor.patch_projector.projectors.0.norm2.bias\n",
      "mfvit.features_processor.patch_projector.projectors.1.norm1.weight\n",
      "mfvit.features_processor.patch_projector.projectors.1.norm1.bias\n",
      "mfvit.features_processor.patch_projector.projectors.1.projector.1.weight\n",
      "mfvit.features_processor.patch_projector.projectors.1.projector.1.bias\n",
      "mfvit.features_processor.patch_projector.projectors.1.projector.4.weight\n",
      "mfvit.features_processor.patch_projector.projectors.1.projector.4.bias\n",
      "mfvit.features_processor.patch_projector.projectors.1.norm2.weight\n",
      "mfvit.features_processor.patch_projector.projectors.1.norm2.bias\n",
      "mfvit.features_processor.patch_projector.projectors.2.norm1.weight\n",
      "mfvit.features_processor.patch_projector.projectors.2.norm1.bias\n",
      "mfvit.features_processor.patch_projector.projectors.2.projector.1.weight\n",
      "mfvit.features_processor.patch_projector.projectors.2.projector.1.bias\n",
      "mfvit.features_processor.patch_projector.projectors.2.projector.4.weight\n",
      "mfvit.features_processor.patch_projector.projectors.2.projector.4.bias\n",
      "mfvit.features_processor.patch_projector.projectors.2.norm2.weight\n",
      "mfvit.features_processor.patch_projector.projectors.2.norm2.bias\n",
      "mfvit.features_processor.patch_projector.projectors.3.norm1.weight\n",
      "mfvit.features_processor.patch_projector.projectors.3.norm1.bias\n",
      "mfvit.features_processor.patch_projector.projectors.3.projector.1.weight\n",
      "mfvit.features_processor.patch_projector.projectors.3.projector.1.bias\n",
      "mfvit.features_processor.patch_projector.projectors.3.projector.4.weight\n",
      "mfvit.features_processor.patch_projector.projectors.3.projector.4.bias\n",
      "mfvit.features_processor.patch_projector.projectors.3.norm2.weight\n",
      "mfvit.features_processor.patch_projector.projectors.3.norm2.bias\n",
      "mfvit.features_processor.patch_projector.projectors.4.norm1.weight\n",
      "mfvit.features_processor.patch_projector.projectors.4.norm1.bias\n",
      "mfvit.features_processor.patch_projector.projectors.4.projector.1.weight\n",
      "mfvit.features_processor.patch_projector.projectors.4.projector.1.bias\n",
      "mfvit.features_processor.patch_projector.projectors.4.projector.4.weight\n",
      "mfvit.features_processor.patch_projector.projectors.4.projector.4.bias\n",
      "mfvit.features_processor.patch_projector.projectors.4.norm2.weight\n",
      "mfvit.features_processor.patch_projector.projectors.4.norm2.bias\n",
      "mfvit.features_processor.patch_projector.projectors.5.norm1.weight\n",
      "mfvit.features_processor.patch_projector.projectors.5.norm1.bias\n",
      "mfvit.features_processor.patch_projector.projectors.5.projector.1.weight\n",
      "mfvit.features_processor.patch_projector.projectors.5.projector.1.bias\n",
      "mfvit.features_processor.patch_projector.projectors.5.projector.4.weight\n",
      "mfvit.features_processor.patch_projector.projectors.5.projector.4.bias\n",
      "mfvit.features_processor.patch_projector.projectors.5.norm2.weight\n",
      "mfvit.features_processor.patch_projector.projectors.5.norm2.bias\n",
      "mfvit.features_processor.patch_projector.projectors.6.norm1.weight\n",
      "mfvit.features_processor.patch_projector.projectors.6.norm1.bias\n",
      "mfvit.features_processor.patch_projector.projectors.6.projector.1.weight\n",
      "mfvit.features_processor.patch_projector.projectors.6.projector.1.bias\n",
      "mfvit.features_processor.patch_projector.projectors.6.projector.4.weight\n",
      "mfvit.features_processor.patch_projector.projectors.6.projector.4.bias\n",
      "mfvit.features_processor.patch_projector.projectors.6.norm2.weight\n",
      "mfvit.features_processor.patch_projector.projectors.6.norm2.bias\n",
      "mfvit.features_processor.patch_projector.projectors.7.norm1.weight\n",
      "mfvit.features_processor.patch_projector.projectors.7.norm1.bias\n",
      "mfvit.features_processor.patch_projector.projectors.7.projector.1.weight\n",
      "mfvit.features_processor.patch_projector.projectors.7.projector.1.bias\n",
      "mfvit.features_processor.patch_projector.projectors.7.projector.4.weight\n",
      "mfvit.features_processor.patch_projector.projectors.7.projector.4.bias\n",
      "mfvit.features_processor.patch_projector.projectors.7.norm2.weight\n",
      "mfvit.features_processor.patch_projector.projectors.7.norm2.bias\n",
      "mfvit.features_processor.patch_projector.projectors.8.norm1.weight\n",
      "mfvit.features_processor.patch_projector.projectors.8.norm1.bias\n",
      "mfvit.features_processor.patch_projector.projectors.8.projector.1.weight\n",
      "mfvit.features_processor.patch_projector.projectors.8.projector.1.bias\n",
      "mfvit.features_processor.patch_projector.projectors.8.projector.4.weight\n",
      "mfvit.features_processor.patch_projector.projectors.8.projector.4.bias\n",
      "mfvit.features_processor.patch_projector.projectors.8.norm2.weight\n",
      "mfvit.features_processor.patch_projector.projectors.8.norm2.bias\n",
      "mfvit.features_processor.patch_projector.projectors.9.norm1.weight\n",
      "mfvit.features_processor.patch_projector.projectors.9.norm1.bias\n",
      "mfvit.features_processor.patch_projector.projectors.9.projector.1.weight\n",
      "mfvit.features_processor.patch_projector.projectors.9.projector.1.bias\n",
      "mfvit.features_processor.patch_projector.projectors.9.projector.4.weight\n",
      "mfvit.features_processor.patch_projector.projectors.9.projector.4.bias\n",
      "mfvit.features_processor.patch_projector.projectors.9.norm2.weight\n",
      "mfvit.features_processor.patch_projector.projectors.9.norm2.bias\n",
      "mfvit.features_processor.patch_projector.projectors.10.norm1.weight\n",
      "mfvit.features_processor.patch_projector.projectors.10.norm1.bias\n",
      "mfvit.features_processor.patch_projector.projectors.10.projector.1.weight\n",
      "mfvit.features_processor.patch_projector.projectors.10.projector.1.bias\n",
      "mfvit.features_processor.patch_projector.projectors.10.projector.4.weight\n",
      "mfvit.features_processor.patch_projector.projectors.10.projector.4.bias\n",
      "mfvit.features_processor.patch_projector.projectors.10.norm2.weight\n",
      "mfvit.features_processor.patch_projector.projectors.10.norm2.bias\n",
      "mfvit.features_processor.patch_projector.projectors.11.norm1.weight\n",
      "mfvit.features_processor.patch_projector.projectors.11.norm1.bias\n",
      "mfvit.features_processor.patch_projector.projectors.11.projector.1.weight\n",
      "mfvit.features_processor.patch_projector.projectors.11.projector.1.bias\n",
      "mfvit.features_processor.patch_projector.projectors.11.projector.4.weight\n",
      "mfvit.features_processor.patch_projector.projectors.11.projector.4.bias\n",
      "mfvit.features_processor.patch_projector.projectors.11.norm2.weight\n",
      "mfvit.features_processor.patch_projector.projectors.11.norm2.bias\n",
      "to_kv.weight\n",
      "to_out.0.weight\n",
      "norm.weight\n",
      "norm.bias\n",
      "cls_head.head.0.weight\n",
      "cls_head.head.0.bias\n",
      "cls_head.head.3.weight\n",
      "cls_head.head.3.bias\n",
      "cls_head.head.6.weight\n",
      "cls_head.head.6.bias\n",
      "semantic_mha.in_proj_weight\n",
      "semantic_mha.in_proj_bias\n",
      "semantic_mha.out_proj.weight\n",
      "semantic_mha.out_proj.bias\n",
      "semantic_projection.weight\n",
      "semantic_projection.bias\n",
      "semantic_layer_norm.weight\n",
      "semantic_layer_norm.bias\n"
     ]
    }
   ],
   "source": [
    "# NEW KEYS: new version of PatchBasedMFViT with the semantic cross-attention parameters\n",
    "new_model_weights = model.state_dict()\n",
    "# print new model weights keys\n",
    "for key in new_model_weights.keys():\n",
    "    print(key)\n",
    "    # if \"vit\" in key:\n",
    "    #     print(key)\n",
    "    #     print(model_weights[key].shape)\n",
    "    #     print(model.state_dict()[key].shape)\n",
    "    #     assert model_weights[key].shape == model.state_dict()[key].shape, f\"Shape mismatch for {key}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f92d68c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the numbers from the new model weights are the same as the old ones\n",
    "for key in model_weights.keys():\n",
    "    # if \"vit\" in key:\n",
    "    #     print(key)\n",
    "    #     print(model_weights[key].shape)\n",
    "    #     print(model.state_dict()[key].shape)\n",
    "    #     assert torch.allclose(\n",
    "    #         model_weights[key], model.state_dict()[key]\n",
    "    #     ), f\"Values mismatch for {key}\"\n",
    "    assert torch.allclose(\n",
    "        model_weights[key], model.state_dict()[key]\n",
    "    ), f\"Values mismatch for {key}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecdc717e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters added to the new model: 58104\n",
      "Number of parameters in the new model: 172213010\n"
     ]
    }
   ],
   "source": [
    "# count the number of parameters added to the new model: new_model_weights - model_weights\n",
    "added_parameters = sum(new_model_weights[key].numel() for key in new_model_weights.keys() if key not in model_weights)\n",
    "\n",
    "print(f\"Number of parameters added to the new model: {added_parameters}\")\n",
    "print(f\"Number of parameters in the new model: {sum(p.numel() for p in model.parameters())}\")\n",
    "\n",
    "# freeze only the original model weights (not the new ones) - the new ones are trainable\n",
    "for name, param in model.named_parameters():\n",
    "\tif name in model_weights:\n",
    "\t\t# print(f\"Freezing {name}\")\n",
    "\t\tparam.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba8edde9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==============================================================================================================\n",
       "Layer (type:depth-idx)                                                                Param #\n",
       "==============================================================================================================\n",
       "PatchBasedMFViT                                                                       1,536\n",
       "MFViT: 1-1                                                                          50,176\n",
       "    CLIPBackbone: 2-1                                                              --\n",
       "        CLIP: 3-1                                                                 (149,620,737)\n",
       "    FrequencyRestorationEstimator: 2-2                                             --\n",
       "        FeatureSpecificProjector: 3-2                                             (22,087,680)\n",
       "    Normalize: 2-3                                                                 --\n",
       "Softmax: 1-2                                                                        --\n",
       "Dropout: 1-3                                                                        --\n",
       "Linear: 1-4                                                                         (221,184)\n",
       "Sequential: 1-5                                                                     --\n",
       "    Linear: 2-4                                                                    (110,592)\n",
       "    Dropout: 2-5                                                                   --\n",
       "LayerNorm: 1-6                                                                      (144)\n",
       "ClassificationHead: 1-7                                                             --\n",
       "    Sequential: 2-6                                                                --\n",
       "        Linear: 3-3                                                               (15,768)\n",
       "        ReLU: 3-4                                                                 --\n",
       "        Dropout: 3-5                                                              --\n",
       "        Linear: 3-6                                                               (46,872)\n",
       "        ReLU: 3-7                                                                 --\n",
       "        Dropout: 3-8                                                              --\n",
       "        Linear: 3-9                                                               (217)\n",
       "MultiheadAttention: 1-8                                                             15,768\n",
       "    NonDynamicallyQuantizableLinear: 2-7                                           5,256\n",
       "Linear: 1-9                                                                         36,936\n",
       "LayerNorm: 1-10                                                                     144\n",
       "==============================================================================================================\n",
       "Total params: 172,213,010\n",
       "Trainable params: 58,104\n",
       "Non-trainable params: 172,154,906\n",
       "=============================================================================================================="
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# Print summary\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e2d05a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
