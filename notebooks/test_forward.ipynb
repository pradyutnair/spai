{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor shape: torch.Size([2, 3, 224, 224])\n",
      "Loading configuration...\n",
      "=> merge config from configs/spai.yaml\n",
      "\n",
      "Creating model...\n",
      "\n",
      "Model Architecture:\n",
      "PatchBasedMFViT(\n",
      "  (mfvit): MFViT(\n",
      "    (vit): CLIPBackbone(\n",
      "      (clip): CLIP(\n",
      "        (visual): VisionTransformer(\n",
      "          (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
      "          (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (transformer): Transformer(\n",
      "            (resblocks): Sequential(\n",
      "              (0): ResidualAttentionBlock(\n",
      "                (attn): MultiheadAttention(\n",
      "                  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "                )\n",
      "                (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Sequential(\n",
      "                  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                  (gelu): QuickGELU()\n",
      "                  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                )\n",
      "                (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "              (1): ResidualAttentionBlock(\n",
      "                (attn): MultiheadAttention(\n",
      "                  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "                )\n",
      "                (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Sequential(\n",
      "                  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                  (gelu): QuickGELU()\n",
      "                  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                )\n",
      "                (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "              (2): ResidualAttentionBlock(\n",
      "                (attn): MultiheadAttention(\n",
      "                  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "                )\n",
      "                (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Sequential(\n",
      "                  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                  (gelu): QuickGELU()\n",
      "                  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                )\n",
      "                (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "              (3): ResidualAttentionBlock(\n",
      "                (attn): MultiheadAttention(\n",
      "                  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "                )\n",
      "                (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Sequential(\n",
      "                  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                  (gelu): QuickGELU()\n",
      "                  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                )\n",
      "                (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "              (4): ResidualAttentionBlock(\n",
      "                (attn): MultiheadAttention(\n",
      "                  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "                )\n",
      "                (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Sequential(\n",
      "                  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                  (gelu): QuickGELU()\n",
      "                  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                )\n",
      "                (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "              (5): ResidualAttentionBlock(\n",
      "                (attn): MultiheadAttention(\n",
      "                  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "                )\n",
      "                (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Sequential(\n",
      "                  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                  (gelu): QuickGELU()\n",
      "                  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                )\n",
      "                (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "              (6): ResidualAttentionBlock(\n",
      "                (attn): MultiheadAttention(\n",
      "                  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "                )\n",
      "                (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Sequential(\n",
      "                  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                  (gelu): QuickGELU()\n",
      "                  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                )\n",
      "                (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "              (7): ResidualAttentionBlock(\n",
      "                (attn): MultiheadAttention(\n",
      "                  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "                )\n",
      "                (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Sequential(\n",
      "                  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                  (gelu): QuickGELU()\n",
      "                  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                )\n",
      "                (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "              (8): ResidualAttentionBlock(\n",
      "                (attn): MultiheadAttention(\n",
      "                  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "                )\n",
      "                (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Sequential(\n",
      "                  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                  (gelu): QuickGELU()\n",
      "                  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                )\n",
      "                (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "              (9): ResidualAttentionBlock(\n",
      "                (attn): MultiheadAttention(\n",
      "                  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "                )\n",
      "                (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Sequential(\n",
      "                  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                  (gelu): QuickGELU()\n",
      "                  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                )\n",
      "                (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "              (10): ResidualAttentionBlock(\n",
      "                (attn): MultiheadAttention(\n",
      "                  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "                )\n",
      "                (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Sequential(\n",
      "                  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                  (gelu): QuickGELU()\n",
      "                  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                )\n",
      "                (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "              (11): ResidualAttentionBlock(\n",
      "                (attn): MultiheadAttention(\n",
      "                  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "                )\n",
      "                (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Sequential(\n",
      "                  (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                  (gelu): QuickGELU()\n",
      "                  (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                )\n",
      "                (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (transformer): Transformer(\n",
      "          (resblocks): Sequential(\n",
      "            (0): ResidualAttentionBlock(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Sequential(\n",
      "                (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (gelu): QuickGELU()\n",
      "                (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (1): ResidualAttentionBlock(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Sequential(\n",
      "                (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (gelu): QuickGELU()\n",
      "                (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (2): ResidualAttentionBlock(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Sequential(\n",
      "                (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (gelu): QuickGELU()\n",
      "                (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (3): ResidualAttentionBlock(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Sequential(\n",
      "                (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (gelu): QuickGELU()\n",
      "                (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (4): ResidualAttentionBlock(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Sequential(\n",
      "                (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (gelu): QuickGELU()\n",
      "                (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (5): ResidualAttentionBlock(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Sequential(\n",
      "                (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (gelu): QuickGELU()\n",
      "                (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (6): ResidualAttentionBlock(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Sequential(\n",
      "                (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (gelu): QuickGELU()\n",
      "                (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (7): ResidualAttentionBlock(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Sequential(\n",
      "                (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (gelu): QuickGELU()\n",
      "                (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (8): ResidualAttentionBlock(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Sequential(\n",
      "                (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (gelu): QuickGELU()\n",
      "                (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (9): ResidualAttentionBlock(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Sequential(\n",
      "                (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (gelu): QuickGELU()\n",
      "                (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (10): ResidualAttentionBlock(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Sequential(\n",
      "                (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (gelu): QuickGELU()\n",
      "                (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (11): ResidualAttentionBlock(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Sequential(\n",
      "                (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (gelu): QuickGELU()\n",
      "                (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (token_embedding): Embedding(49408, 512)\n",
      "        (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (features_processor): FrequencyRestorationEstimator(\n",
      "      (semantic_fusion): SemanticFusionModule(\n",
      "        (gate): Sequential(\n",
      "          (0): Linear(in_features=1536, out_features=1024, bias=True)\n",
      "          (1): Sigmoid()\n",
      "        )\n",
      "      )\n",
      "      (patch_projector): FeatureSpecificProjector(\n",
      "        (projectors): ModuleList(\n",
      "          (0-11): 12 x Projector(\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (projector): Sequential(\n",
      "              (0): Dropout(p=0.5, inplace=False)\n",
      "              (1): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (2): GELU(approximate='none')\n",
      "              (3): Dropout(p=0.5, inplace=False)\n",
      "              (4): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (5): Identity()\n",
      "              (6): Dropout(p=0.5, inplace=False)\n",
      "            )\n",
      "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (original_features_processor): FeatureImportanceProjector(\n",
      "        (proj1): Projector(\n",
      "          (norm1): Identity()\n",
      "          (projector): Sequential(\n",
      "            (0): Dropout(p=0.5, inplace=False)\n",
      "            (1): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "            (2): GELU(approximate='none')\n",
      "            (3): Dropout(p=0.5, inplace=False)\n",
      "            (4): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (5): GELU(approximate='none')\n",
      "            (6): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (proj2): Projector(\n",
      "          (norm1): Identity()\n",
      "          (projector): Sequential(\n",
      "            (0): Dropout(p=0.5, inplace=False)\n",
      "            (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (2): GELU(approximate='none')\n",
      "            (3): Dropout(p=0.5, inplace=False)\n",
      "            (4): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (5): GELU(approximate='none')\n",
      "            (6): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (backbone_norm): Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
      "  )\n",
      "  (attend): Softmax(dim=-1)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (to_kv): Linear(in_features=1096, out_features=3072, bias=False)\n",
      "  (to_out): Sequential(\n",
      "    (0): Linear(in_features=1536, out_features=1096, bias=False)\n",
      "    (1): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (norm): LayerNorm((1096,), eps=1e-05, elementwise_affine=True)\n",
      "  (cls_head): ClassificationHead(\n",
      "    (head): Sequential(\n",
      "      (0): Linear(in_features=1096, out_features=3288, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Dropout(p=0.5, inplace=False)\n",
      "      (3): Linear(in_features=3288, out_features=3288, bias=True)\n",
      "      (4): ReLU()\n",
      "      (5): Dropout(p=0.5, inplace=False)\n",
      "      (6): Linear(in_features=3288, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "Model Parameters:\n",
      "<generator object Module.parameters at 0x2bfe2eea0>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "# Change path\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "os.chdir(os.path.dirname(os.getcwd()))\n",
    "# # Get the notebook directory\n",
    "# notebook_dir = os.path.dirname(os.getcwd())\n",
    "# print(notebook_dir)\n",
    "# sys.path.append(\"/Users/pradyut.nair/Documents/University/DL2/spai/spai\")\n",
    "# os.chdir(\"/Users/pradyut.nair/Documents/University/DL2/spai/spai\")\n",
    "# Go to build.py file\n",
    "from spai.models.build import build_cls_model  # Change this import\n",
    "from spai.config import get_config\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Create a random input tensor with shape (batch_size, channels, height, width)\n",
    "batch_size = 2\n",
    "img_size = 224\n",
    "input_tensor = torch.randn(batch_size, 3, img_size, img_size)\n",
    "print(f\"Input tensor shape: {input_tensor.shape}\")\n",
    "\n",
    "# Normalize input to [0, 1] range\n",
    "input_tensor = torch.clamp(input_tensor, min=0., max=1.)\n",
    "\n",
    "# Load config and create model with semantic fusion enabled\n",
    "print(\"Loading configuration...\")\n",
    "config = get_config({\"cfg\": \"configs/spai.yaml\"})\n",
    "config.defrost()\n",
    "config.MODEL.FRE.USE_SEMANTIC_FUSION = True\n",
    "config.MODEL_WEIGHTS = \"clip\"  # Use CLIP backbone for semantic features\n",
    "config.freeze()\n",
    "\n",
    "print(\"\\nCreating model...\")\n",
    "model = build_cls_model(config)  # Use build_cls_model instead of build_model\n",
    "\n",
    "# Print model architecture\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(model)\n",
    "\n",
    "# Print model parameters\n",
    "print(\"\\nModel Parameters:\")\n",
    "print(model.parameters())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projector input shape: torch.Size([2, 196, 768])\n",
      "Projector input shape: torch.Size([2, 196, 768])\n",
      "Projector input shape: torch.Size([2, 196, 768])\n",
      "Projector input shape: torch.Size([2, 196, 768])\n",
      "Projector input shape: torch.Size([2, 196, 768])\n",
      "Projector input shape: torch.Size([2, 196, 768])\n",
      "Projector input shape: torch.Size([2, 196, 768])\n",
      "Projector input shape: torch.Size([2, 196, 768])\n",
      "Projector input shape: torch.Size([2, 196, 768])\n",
      "Projector input shape: torch.Size([2, 196, 768])\n",
      "Projector input shape: torch.Size([2, 196, 768])\n",
      "Projector input shape: torch.Size([2, 196, 768])\n",
      "Projector input shape: torch.Size([2, 196, 768])\n",
      "Projector input shape: torch.Size([2, 196, 768])\n",
      "Projector input shape: torch.Size([2, 196, 768])\n",
      "Projector input shape: torch.Size([2, 196, 768])\n",
      "Projector input shape: torch.Size([2, 196, 768])\n",
      "Projector input shape: torch.Size([2, 196, 768])\n",
      "Projector input shape: torch.Size([2, 196, 768])\n",
      "Projector input shape: torch.Size([2, 196, 768])\n",
      "Projector input shape: torch.Size([2, 196, 768])\n",
      "Projector input shape: torch.Size([2, 196, 768])\n",
      "Projector input shape: torch.Size([2, 196, 768])\n",
      "Projector input shape: torch.Size([2, 196, 768])\n",
      "Projector input shape: torch.Size([2, 196, 768])\n",
      "Projector input shape: torch.Size([2, 196, 768])\n",
      "Projector input shape: torch.Size([2, 196, 768])\n",
      "Projector input shape: torch.Size([2, 196, 768])\n",
      "Projector input shape: torch.Size([2, 196, 768])\n",
      "Projector input shape: torch.Size([2, 196, 768])\n",
      "Projector input shape: torch.Size([2, 196, 768])\n",
      "Projector input shape: torch.Size([2, 196, 768])\n",
      "Projector input shape: torch.Size([2, 196, 768])\n",
      "Projector input shape: torch.Size([2, 196, 768])\n",
      "Projector input shape: torch.Size([2, 196, 768])\n",
      "Projector input shape: torch.Size([2, 196, 768])\n",
      "Projector input shape: torch.Size([24, 2048])\n",
      "Projector input shape: torch.Size([2, 1024])\n",
      "Output shape (fixed resolution): torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn((batch_size, 3, 224, 224))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(x)\n",
    "\n",
    "print(f\"Output shape (fixed resolution): {out.shape}\")\n",
    "assert out.shape == (batch_size, 1), f\"Unexpected output shape: {out.shape}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape (fixed resolution): torch.Size([2, 1])\n",
      "\n",
      "Semantic Fusion Parameters:\n",
      "\n",
      "gate.0.weight:\n",
      "Shape: torch.Size([1024, 1536])\n",
      "Values: tensor([[ 0.0107,  0.0163, -0.0022,  ...,  0.0003,  0.0308, -0.0242],\n",
      "        [-0.0134, -0.0118, -0.0066,  ...,  0.0078,  0.0050, -0.0368],\n",
      "        [ 0.0347,  0.0403,  0.0004,  ..., -0.0207,  0.0086,  0.0022],\n",
      "        ...,\n",
      "        [-0.0092, -0.0209, -0.0061,  ..., -0.0088, -0.0093, -0.0051],\n",
      "        [ 0.0094,  0.0019,  0.0273,  ..., -0.0093, -0.0251, -0.0036],\n",
      "        [-0.0097,  0.0006, -0.0102,  ...,  0.0085, -0.0444, -0.0202]])\n",
      "\n",
      "gate.0.bias:\n",
      "Shape: torch.Size([1024])\n",
      "Values: tensor([0., 0., 0.,  ..., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# ... existing code ...\n",
    "print(f\"Output shape (fixed resolution): {out.shape}\")\n",
    "assert out.shape == (batch_size, 1), f\"Unexpected output shape: {out.shape}\"\n",
    "\n",
    "# Print semantic fusion parameters\n",
    "print(\"\\nSemantic Fusion Parameters:\")\n",
    "semantic_fusion = model.mfvit.features_processor.semantic_fusion\n",
    "for name, param in semantic_fusion.named_parameters():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"Shape: {param.shape}\")\n",
    "    print(f\"Values: {param.data}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
